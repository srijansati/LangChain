{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SINCE THE QUERY VECTOR STORE IS NOT THAT EFFICIENT SO WE WILL BE USING 'CHAIN AND RETRIEVER' WHICH USES LLM MODELS FOR GENERATING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 0}, page_content=\" \\nEnd to End Text Summarization: A modular  \\nApproach using Hugging Face Transformers\\nAman Sharma  \\nData Science and Engineering \\nManipal University Jaipur \\nRajisthan, India \\naman.229303072@muj.manipal.edu \\nSrijan Sati \\nData Science and Engineering \\nManipal University Jaipur \\nRajisthan, India \\nsrijan.229309001@muj.manipal.edu \\nAbstract— This paper presents a versatile and scalable \\nsystem for technical text summarization, designed to address the \\ngrowing demand for effective summarization in specialized \\nfields. The system uses Hugging Face's Pegasus Transformer \\nmodel to produce coherent and  contextually accurate \\nabstractive summaries. Built with Python's object -oriented \\nprogramming principles, it incorporates essential functions \\nsuch as data ingestion, preprocessing, tokenization, and model \\nexecution. Optimized through Continuous I ntegration and \\nDeployment (CI/CD) pipelines, the workflow ensures efficiency \\nand scalability. The system's performance is assessed using \\nROUGE metrics, highlighting its practical applicability. By \\ncombining advanced machine learning methods with robust \\nsoftware engineering practices, this work offers a scalable \\nsolution for summarizing complex technical content... \\nI. INTRODUCTION \\nWith the vast increase in information, efficiently \\nsummarizing text has become the need of the day to extract \\nvaluable insights from lengthy documents. Technical \\nsummarization, or focused condensation of intricate technical \\ninformation, captures key inform ation pertinent to more \\ninsightful understanding and decision making. It is highly \\nbeneficial to those information -intensive professionals who \\nenhance their productivity as well as the accessibility of \\nrelevant information at the same time. While there hav e \\nindeed been tremendous breakthroughs related to NLP, \\nbuilding summarization systems that lead to scalability, \\nflexibility, and ease of maintenance remains a complex issue. \\nThis paper presents a technical summarization solution that is \\nmodular as well as pipeline-oriented using the Pegasus \\nTransformer model developed by Hugging Face. Utilities \\nemployed in the system are based on Python for data \\ningestion, preprocessing, and model refinement and are \\nsupplemented with Continuous Integration and Deployment \\n(CI/CD) to test, validate, and deploy efficiently. The \\nframework suggested in this paper seeks to solve challenges \\nfacing technical summarization across domains: an effective \\nand scalable approach for dealing with complex technical \\ndocuments... \\nII. LITERATURE REVIEW \\nText summarization  Involves the reduction of lengthy texts \\ninto concise length, preserving key information. There exist \\ntwo approaches: extractive summarization-the selection of \\nkey sentences taken directly from the source-and abstractive \\nsummarization-new sentences generated to convey the main \\nideas. While abstractive-based methods bring improved \\ncoherence and precision in texts, which can prove very \\neffective in specialized fields such as research papers or \\ntechnical manuals, methods such as extractive are much \\nsimpler and less resource-intensive. \\nHugging Face has many pre-trained Transformer models, \\namong which Pegasus and BERT and GPT stand out with \\nself-attention mechanisms and large-scale pretraining; they \\ndefinitely shine on the tasks of summarization, sentiment \\nanalysis, and text generation. The API is intuitive enough to \\nbe integrated into, fine-tuned, and customized, making it a \\nvery valuable resource for building summarization \\nworkflows. \\nNLP systems implement modular programming, which \\nencourages more scalability and ease of maintenance, \\nbreaking complex systems into independent components. \\nThis design makes it easier to debug, add new features, and \\nreuse common functionalities. The high support offered by \\nPython for object-oriented approach makes designing \\nmodular components, like any sort of data ingestion and \\npreprocessing process, easier while keeping the system \\nflexible and robust. \\nCI/CD pipelines automate the testing and validation and \\ndeployments process to improve the efficiency of the \\nsoftware development and reliability. Continuous Integration \\nin ensuring smooth change integration while continuous \\ndeployment, updates to the prod uction lines are automated. \\nThese will, therefore, maintain high levels of efficiency and \\nscale ideals in projects on NLP. In this system, the CI/CD \\npipeline ensures efficient and smooth testing and deployment \\nwith improvements in workflows all the time... \\nIII. METHODOLOGY \\nA. Project overview  \\nThis project implements a structured and modular \\napproach toward the creation of a holistic, complete, and \\nwell-rounded pipeline in summarizing technical texts. The \\nsystem should be scalable, maintainable, and adaptable due \\nto the nature of process design at multiple stages starting from \\ninitial setting to final deployment. Standing at the center of \\nthis pipeline is the Pegasus Transformer model, which is a \\nstate-of-the-art model reputed for producing superior \"),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 1}, page_content=\"performances in abstractive summarization, especially on \\ntechnical materials. To optimize and fine -tune the resource -\\nintensive training phase, the process is optimized to run on \\nGPU environments, which are appropriately crucial for \\nmanaging the demands deep learning. The paper emphasizes \\nthe fine -tuning procedure of Pegasus, demonstrating its \\ncapacity in creating high -quality summaries, subject to \\ntechnical domains as defined by technical datasets. \\nB. Dataset Collection \\nFor data collection, I have selected Samsung Tech Dialog \\ndataset, which is explicitly designed to be used for dialog -\\nbased text and summarization tasks. The fact that this dataset \\nconsists of rich technical contents leads me to strongly \\nbelieve that it cou ld be well applied to abstractive \\nsummarization. Once I proceeded by downloading the data \\nand then getting them organized while conducting an \\nextensive validation to ensure the dataset met proper quality \\nstandards. To optimize the ingestion workflow, I \\nimplemented schema validations and used Python's pathlib \\nlibrary to handle flexible path management, whereby the \\ndataset could easily be accessed and processed across \\ndifferent environments. \\nC. Data Preparation \\nIn this data preparation process, my main goal was to \\nprepare the dataset for optimal training of the model. \\nTransformation of raw texts into a tokenized format where \\nwords and phrases were converted into numerical \\nrepresentations belongs to this process. I also created \\nattention masks that guide the model's attention to some other \\nrelevant parts of input texts in training time. Additionally, I \\nutilized efficient data batching techniques to minimize the \\nutilization of memory as well as increase the training  speed \\nwhile at the same time maintaining the data. All these pre -\\nprocessing steps ensured that the dataset was clean and fitted \\ninto the Pegasus Transformer model. \\nD. Model Finetuning \\nDuring the adaptation phase of the model to fine -tune a \\npre-trained model for summary tasks on technical texts, I \\nused the transformers library from Hugging Face. Pegasus is \\napplied using a batch size of 16, learning rate at 5e -5, and an \\noverall training epoch of three. The optimization process used \\nthe cross -entropy loss function, which is effective, \\nparticularly for the generation type of text tasks. On \\nperformance evaluation, I used the ROUGE metrics after \\ntraining, so that the produced summaries were co herent, \\nrelevant, and of high quality. This step was quite important \\nfor customizing the model towards producing effective \\nsummaries for technical documents. \\nE. Deployment \\nDocker provides for efficient deployment by containerizing \\nthe model and its dependencies to become a portable and \\nunified application, and CI/CD pipelines are employed for \\nautomating processes such as testing, validation, and \\ndeployment to the cloud, so as to ensure the system is reliable \\nand ready for production. Such workflows help achieve \\nspeedy updates and scalability and ensure the system is apt \\nand proper for practical applications in real life \\nF. Result \\nThe Samsum dataset, known for challenging conversational \\nsummarization systems, was chosen to test the model's \\nability. In this approach, the model was fine -tuned to enable \\nit to yield adequate and coherent summaries that have a \\nfaithful representation of the centra l points of discussions. \\nIndeed, good ROUGE scores result for the model, proving \\nthat it can retain critical information and output high -quality \\nsummaries. Therefore, these results point out the system's \\nefficiency in compressing dialogue data, maintaining  \\ncontextual integrity, and proving its effectiveness for the \\nsummarization of conversational content. \\nOne of its main factors for success is that this model boasts \\n50% efficiency compared to CPU -based processing. Its \\nmodular architecture divided tasks such as data ingestion, \\npreprocessing, and model training, which were easier to \\ndebug and afforded improve d reusability -this made it quite \\nadaptable to a wide variety of NLP tasks. Optimizations like \\nbatching and attention masks for example, enabled the system \\nto process larger datasets without hitting memory limits. \\n \\nAlthough this is the case, significant challenges the project \\nfaces have originated from this time. This time concerns \\nimbalanced data within the Samsum dataset, resulting in the \\nquality of summaries being bad. Techniques in preprocessing \\nare in use here, but further refinements are called for. \\nAdditionally, the computational requirement within the \\nTransformer model also poses challenges that may be \\nresolved with more powerful hardware or through distributed \\ntraining approaches. The dataset was conversation al by \\nnature, which added a complexity with respect to the model \\nhaving to deal with multi -turn contexts. Hence, it is much \\nharder than typical document summarization. \\n \\nThe system was considered both for development efficiency \\nas well as to create highly accurate summaries. Its modular \\ndesign provided flexibility for easy addition of new features \\nor data sets. Future improvements would be with handling \\nfiner nuances of co nversation by the model and with \\nincreased computational efficiency. \\n \\n \\n \\n\"),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 2}, page_content=\"IV. CONCLUSION \\nThe research presents an end-to-end technical summarization \\nsystem designed with a modular approach to enhance \\nscalability, flexibility, and long -term maintainability. \\nUtilizing Hugging Face Transformers, the system delivers \\nhigh-quality, precise text summ aries. Its modular structure \\nallows for easy updates, ensuring sustainability in production \\nenvironments. Integration of CI/CD practices optimizes the \\ndevelopment process, shortening time -to-market and \\nenabling seamless deployment. Capable of processing la rge-\\nscale technical content, the system provides a powerful tool \\nfor summarizing complex documents. Future improvements \\nwill focus on expanding datasets, enhancing the model's \\nabstractive capabilities, and adding multilingual support to \\nincrease its reach across diverse languages and regions. \\nV. ACKNOWLEDGVMENTS   \\nI would like to thank Mrs. Neha V Sharma for giving me \\nthis opportunity to implement the concepts of Natural \\nLanguage Processing in this paper and for her guidance. \\nREFERENCES \\n[1] Hugging Face Inc. (2024). Transformers Library Documentation . \\n[2] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, \\nA. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention Is All You Need. \\nIn Proceedings of NeurIPS 2017. \\n \\n[3] Lin, C. -Y. (2004). ROUGE: A Package for Automatic Evaluation of \\nSummaries. In Proceedings of the ACL Workshop on Text \\nSummarization  \\n[4]  Docker, Inc. (2024). Docker Documentation. \\n[5]   Raffel, C., Shinn, D., Roberts, A., Lee, S., & Narang, S. (2020). \\nExploring the Limits of Transfer Learning with a Unified Text-to-Text \\nTransformer (T5). Journal of Machine Learning Research, 21(1), 1-67. \\n[6 https://statisticsglobe.com/text-summarization-hugging-face-\\ntransformers-python \\n \\n \\n \")]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path= \"SrijanSati_229309001.pdf\")\n",
    "text_pdf = loader.load()\n",
    "text_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 0}, page_content=\"End to End Text Summarization: A modular  \\nApproach using Hugging Face Transformers\\nAman Sharma  \\nData Science and Engineering \\nManipal University Jaipur \\nRajisthan, India \\naman.229303072@muj.manipal.edu \\nSrijan Sati \\nData Science and Engineering \\nManipal University Jaipur \\nRajisthan, India \\nsrijan.229309001@muj.manipal.edu \\nAbstract— This paper presents a versatile and scalable \\nsystem for technical text summarization, designed to address the \\ngrowing demand for effective summarization in specialized \\nfields. The system uses Hugging Face's Pegasus Transformer \\nmodel to produce coherent and  contextually accurate \\nabstractive summaries. Built with Python's object -oriented \\nprogramming principles, it incorporates essential functions \\nsuch as data ingestion, preprocessing, tokenization, and model \\nexecution. Optimized through Continuous I ntegration and \\nDeployment (CI/CD) pipelines, the workflow ensures efficiency \\nand scalability. The system's performance is assessed using\"),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 0}, page_content=\"execution. Optimized through Continuous I ntegration and \\nDeployment (CI/CD) pipelines, the workflow ensures efficiency \\nand scalability. The system's performance is assessed using \\nROUGE metrics, highlighting its practical applicability. By \\ncombining advanced machine learning methods with robust \\nsoftware engineering practices, this work offers a scalable \\nsolution for summarizing complex technical content... \\nI. INTRODUCTION \\nWith the vast increase in information, efficiently \\nsummarizing text has become the need of the day to extract \\nvaluable insights from lengthy documents. Technical \\nsummarization, or focused condensation of intricate technical \\ninformation, captures key inform ation pertinent to more \\ninsightful understanding and decision making. It is highly \\nbeneficial to those information -intensive professionals who \\nenhance their productivity as well as the accessibility of \\nrelevant information at the same time. While there hav e\"),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 0}, page_content='beneficial to those information -intensive professionals who \\nenhance their productivity as well as the accessibility of \\nrelevant information at the same time. While there hav e \\nindeed been tremendous breakthroughs related to NLP, \\nbuilding summarization systems that lead to scalability, \\nflexibility, and ease of maintenance remains a complex issue. \\nThis paper presents a technical summarization solution that is \\nmodular as well as pipeline-oriented using the Pegasus \\nTransformer model developed by Hugging Face. Utilities \\nemployed in the system are based on Python for data \\ningestion, preprocessing, and model refinement and are \\nsupplemented with Continuous Integration and Deployment \\n(CI/CD) to test, validate, and deploy efficiently. The \\nframework suggested in this paper seeks to solve challenges \\nfacing technical summarization across domains: an effective \\nand scalable approach for dealing with complex technical \\ndocuments... \\nII. LITERATURE REVIEW'),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 0}, page_content='facing technical summarization across domains: an effective \\nand scalable approach for dealing with complex technical \\ndocuments... \\nII. LITERATURE REVIEW \\nText summarization  Involves the reduction of lengthy texts \\ninto concise length, preserving key information. There exist \\ntwo approaches: extractive summarization-the selection of \\nkey sentences taken directly from the source-and abstractive \\nsummarization-new sentences generated to convey the main \\nideas. While abstractive-based methods bring improved \\ncoherence and precision in texts, which can prove very \\neffective in specialized fields such as research papers or \\ntechnical manuals, methods such as extractive are much \\nsimpler and less resource-intensive. \\nHugging Face has many pre-trained Transformer models, \\namong which Pegasus and BERT and GPT stand out with \\nself-attention mechanisms and large-scale pretraining; they \\ndefinitely shine on the tasks of summarization, sentiment'),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 0}, page_content='among which Pegasus and BERT and GPT stand out with \\nself-attention mechanisms and large-scale pretraining; they \\ndefinitely shine on the tasks of summarization, sentiment \\nanalysis, and text generation. The API is intuitive enough to \\nbe integrated into, fine-tuned, and customized, making it a \\nvery valuable resource for building summarization \\nworkflows. \\nNLP systems implement modular programming, which \\nencourages more scalability and ease of maintenance, \\nbreaking complex systems into independent components. \\nThis design makes it easier to debug, add new features, and \\nreuse common functionalities. The high support offered by \\nPython for object-oriented approach makes designing \\nmodular components, like any sort of data ingestion and \\npreprocessing process, easier while keeping the system \\nflexible and robust. \\nCI/CD pipelines automate the testing and validation and \\ndeployments process to improve the efficiency of the \\nsoftware development and reliability. Continuous Integration'),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 0}, page_content='flexible and robust. \\nCI/CD pipelines automate the testing and validation and \\ndeployments process to improve the efficiency of the \\nsoftware development and reliability. Continuous Integration \\nin ensuring smooth change integration while continuous \\ndeployment, updates to the prod uction lines are automated. \\nThese will, therefore, maintain high levels of efficiency and \\nscale ideals in projects on NLP. In this system, the CI/CD \\npipeline ensures efficient and smooth testing and deployment \\nwith improvements in workflows all the time... \\nIII. METHODOLOGY \\nA. Project overview  \\nThis project implements a structured and modular \\napproach toward the creation of a holistic, complete, and \\nwell-rounded pipeline in summarizing technical texts. The \\nsystem should be scalable, maintainable, and adaptable due \\nto the nature of process design at multiple stages starting from \\ninitial setting to final deployment. Standing at the center of'),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 0}, page_content='system should be scalable, maintainable, and adaptable due \\nto the nature of process design at multiple stages starting from \\ninitial setting to final deployment. Standing at the center of \\nthis pipeline is the Pegasus Transformer model, which is a \\nstate-of-the-art model reputed for producing superior'),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 1}, page_content='performances in abstractive summarization, especially on \\ntechnical materials. To optimize and fine -tune the resource -\\nintensive training phase, the process is optimized to run on \\nGPU environments, which are appropriately crucial for \\nmanaging the demands deep learning. The paper emphasizes \\nthe fine -tuning procedure of Pegasus, demonstrating its \\ncapacity in creating high -quality summaries, subject to \\ntechnical domains as defined by technical datasets. \\nB. Dataset Collection \\nFor data collection, I have selected Samsung Tech Dialog \\ndataset, which is explicitly designed to be used for dialog -\\nbased text and summarization tasks. The fact that this dataset \\nconsists of rich technical contents leads me to strongly \\nbelieve that it cou ld be well applied to abstractive \\nsummarization. Once I proceeded by downloading the data \\nand then getting them organized while conducting an \\nextensive validation to ensure the dataset met proper quality'),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 1}, page_content=\"summarization. Once I proceeded by downloading the data \\nand then getting them organized while conducting an \\nextensive validation to ensure the dataset met proper quality \\nstandards. To optimize the ingestion workflow, I \\nimplemented schema validations and used Python's pathlib \\nlibrary to handle flexible path management, whereby the \\ndataset could easily be accessed and processed across \\ndifferent environments. \\nC. Data Preparation \\nIn this data preparation process, my main goal was to \\nprepare the dataset for optimal training of the model. \\nTransformation of raw texts into a tokenized format where \\nwords and phrases were converted into numerical \\nrepresentations belongs to this process. I also created \\nattention masks that guide the model's attention to some other \\nrelevant parts of input texts in training time. Additionally, I \\nutilized efficient data batching techniques to minimize the \\nutilization of memory as well as increase the training  speed\"),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 1}, page_content='relevant parts of input texts in training time. Additionally, I \\nutilized efficient data batching techniques to minimize the \\nutilization of memory as well as increase the training  speed \\nwhile at the same time maintaining the data. All these pre -\\nprocessing steps ensured that the dataset was clean and fitted \\ninto the Pegasus Transformer model. \\nD. Model Finetuning \\nDuring the adaptation phase of the model to fine -tune a \\npre-trained model for summary tasks on technical texts, I \\nused the transformers library from Hugging Face. Pegasus is \\napplied using a batch size of 16, learning rate at 5e -5, and an \\noverall training epoch of three. The optimization process used \\nthe cross -entropy loss function, which is effective, \\nparticularly for the generation type of text tasks. On \\nperformance evaluation, I used the ROUGE metrics after \\ntraining, so that the produced summaries were co herent, \\nrelevant, and of high quality. This step was quite important'),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 1}, page_content=\"performance evaluation, I used the ROUGE metrics after \\ntraining, so that the produced summaries were co herent, \\nrelevant, and of high quality. This step was quite important \\nfor customizing the model towards producing effective \\nsummaries for technical documents. \\nE. Deployment \\nDocker provides for efficient deployment by containerizing \\nthe model and its dependencies to become a portable and \\nunified application, and CI/CD pipelines are employed for \\nautomating processes such as testing, validation, and \\ndeployment to the cloud, so as to ensure the system is reliable \\nand ready for production. Such workflows help achieve \\nspeedy updates and scalability and ensure the system is apt \\nand proper for practical applications in real life \\nF. Result \\nThe Samsum dataset, known for challenging conversational \\nsummarization systems, was chosen to test the model's \\nability. In this approach, the model was fine -tuned to enable \\nit to yield adequate and coherent summaries that have a\"),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 1}, page_content=\"summarization systems, was chosen to test the model's \\nability. In this approach, the model was fine -tuned to enable \\nit to yield adequate and coherent summaries that have a \\nfaithful representation of the centra l points of discussions. \\nIndeed, good ROUGE scores result for the model, proving \\nthat it can retain critical information and output high -quality \\nsummaries. Therefore, these results point out the system's \\nefficiency in compressing dialogue data, maintaining  \\ncontextual integrity, and proving its effectiveness for the \\nsummarization of conversational content. \\nOne of its main factors for success is that this model boasts \\n50% efficiency compared to CPU -based processing. Its \\nmodular architecture divided tasks such as data ingestion, \\npreprocessing, and model training, which were easier to \\ndebug and afforded improve d reusability -this made it quite \\nadaptable to a wide variety of NLP tasks. Optimizations like\"),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 1}, page_content='preprocessing, and model training, which were easier to \\ndebug and afforded improve d reusability -this made it quite \\nadaptable to a wide variety of NLP tasks. Optimizations like \\nbatching and attention masks for example, enabled the system \\nto process larger datasets without hitting memory limits. \\n \\nAlthough this is the case, significant challenges the project \\nfaces have originated from this time. This time concerns \\nimbalanced data within the Samsum dataset, resulting in the \\nquality of summaries being bad. Techniques in preprocessing \\nare in use here, but further refinements are called for. \\nAdditionally, the computational requirement within the \\nTransformer model also poses challenges that may be \\nresolved with more powerful hardware or through distributed \\ntraining approaches. The dataset was conversation al by \\nnature, which added a complexity with respect to the model \\nhaving to deal with multi -turn contexts. Hence, it is much \\nharder than typical document summarization.'),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 1}, page_content='nature, which added a complexity with respect to the model \\nhaving to deal with multi -turn contexts. Hence, it is much \\nharder than typical document summarization. \\n \\nThe system was considered both for development efficiency \\nas well as to create highly accurate summaries. Its modular \\ndesign provided flexibility for easy addition of new features \\nor data sets. Future improvements would be with handling \\nfiner nuances of co nversation by the model and with \\nincreased computational efficiency.'),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 2}, page_content=\"IV. CONCLUSION \\nThe research presents an end-to-end technical summarization \\nsystem designed with a modular approach to enhance \\nscalability, flexibility, and long -term maintainability. \\nUtilizing Hugging Face Transformers, the system delivers \\nhigh-quality, precise text summ aries. Its modular structure \\nallows for easy updates, ensuring sustainability in production \\nenvironments. Integration of CI/CD practices optimizes the \\ndevelopment process, shortening time -to-market and \\nenabling seamless deployment. Capable of processing la rge-\\nscale technical content, the system provides a powerful tool \\nfor summarizing complex documents. Future improvements \\nwill focus on expanding datasets, enhancing the model's \\nabstractive capabilities, and adding multilingual support to \\nincrease its reach across diverse languages and regions. \\nV. ACKNOWLEDGVMENTS   \\nI would like to thank Mrs. Neha V Sharma for giving me \\nthis opportunity to implement the concepts of Natural\"),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 2}, page_content='increase its reach across diverse languages and regions. \\nV. ACKNOWLEDGVMENTS   \\nI would like to thank Mrs. Neha V Sharma for giving me \\nthis opportunity to implement the concepts of Natural \\nLanguage Processing in this paper and for her guidance. \\nREFERENCES \\n[1] Hugging Face Inc. (2024). Transformers Library Documentation . \\n[2] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, \\nA. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention Is All You Need. \\nIn Proceedings of NeurIPS 2017. \\n \\n[3] Lin, C. -Y. (2004). ROUGE: A Package for Automatic Evaluation of \\nSummaries. In Proceedings of the ACL Workshop on Text \\nSummarization  \\n[4]  Docker, Inc. (2024). Docker Documentation. \\n[5]   Raffel, C., Shinn, D., Roberts, A., Lee, S., & Narang, S. (2020). \\nExploring the Limits of Transfer Learning with a Unified Text-to-Text \\nTransformer (T5). Journal of Machine Learning Research, 21(1), 1-67. \\n[6 https://statisticsglobe.com/text-summarization-hugging-face-'),\n",
       " Document(metadata={'source': 'SrijanSati_229309001.pdf', 'page': 2}, page_content='Transformer (T5). Journal of Machine Learning Research, 21(1), 1-67. \\n[6 https://statisticsglobe.com/text-summarization-hugging-face-\\ntransformers-python')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "documents = text_splitter.split_documents(documents= text_pdf)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a vector store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "db = FAISS.from_documents(documents= documents, embedding= OllamaEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the llm model\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model = \"llama2\")\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#design the prompt template\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                          \n",
    "Answer the following question based only on the provided context.\n",
    "Think step by step before answering the question.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\n",
    "                                          \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain and stuff_document_chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. \n",
    "A retriever does not need to be able to store documents, only to return (or retrieve) them. \n",
    "Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.\n",
    "Retrievers accept a string query as input and return a list of Document's as output.'''\n",
    "\n",
    "#creating a retriever\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a retrival chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrival_chain = create_retrieval_chain(retriever, document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a response using LLM\n",
    "response = retrival_chain.invoke({\"input\": \"What is Hugging Face\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, Hugging Face is a company or organization that develops and maintains the Transformers library, which is a state-of-the-art model for producing superior summaries.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finally returning the response\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
